{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SpecAugment import spec_augment_pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa.display\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = np.load(\"../audioData/train_process_mel/00716-3-005-m-24-335-sad-sad-sad.npy\")\n",
    "mel_spectrogram = torch.Tensor([mel_spectrogram]*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.3 ms, sys: 18.5 ms, total: 39.8 ms\n",
      "Wall time: 6.87 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-29.8867, -19.4721, -20.1508,  ..., -12.6657, -21.7644, -28.3287],\n",
       "         [-42.2429, -21.4899, -21.3133,  ..., -17.6198, -21.5316, -21.7439],\n",
       "         [-23.1295, -21.4466, -22.5453,  ..., -21.4243, -21.6229, -20.2691],\n",
       "         ...,\n",
       "         [-56.7934, -58.9579, -59.0702,  ..., -60.1270, -61.1168, -62.0487],\n",
       "         [-58.4937, -57.1383, -56.6848,  ..., -62.2462, -65.4136, -63.1610],\n",
       "         [-57.5767, -54.8568, -55.1241,  ..., -63.3680, -65.5047, -65.5047]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spec_augment_pytorch.spec_augment(mel_spectrogram[4].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 93])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sa(i_mel):\n",
    "    return spec_augment_pytorch.spec_augment(i_mel,time_warping_para=80, frequency_masking_para=15,\n",
    "                 time_masking_para=15, frequency_mask_num=2, time_mask_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alpha = torch.tensor(queries - floor, dtype=grid_type)\n",
      "/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alpha = torch.tensor(queries - floor, dtype=grid_type)\n",
      "/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alpha = torch.tensor(queries - floor, dtype=grid_type)\n",
      "/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alpha = torch.tensor(queries - floor, dtype=grid_type)\n",
      "/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alpha = torch.tensor(queries - floor, dtype=grid_type)\n",
      "/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alpha = torch.tensor(queries - floor, dtype=grid_type)\n",
      "/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alpha = torch.tensor(queries - floor, dtype=grid_type)\n",
      "/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alpha = torch.tensor(queries - floor, dtype=grid_type)\n",
      "/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alpha = torch.tensor(queries - floor, dtype=grid_type)\n",
      "/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  alpha = torch.tensor(queries - floor, dtype=grid_type)\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c7de4f1d2ebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mi_mel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mi_mels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi_mels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         '''\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "B = mel_spectrogram.shape[0]\n",
    "i_mels = []\n",
    "for i in range(B):\n",
    "    i_mel = mel_spectrogram[i,:,:].unsqueeze(0)\n",
    "    i_mels.append(i_mel)\n",
    "warped = pool.map(sa,i_mels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.86 s, sys: 0 ns, total: 1.86 s\n",
      "Wall time: 218 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "B = mel_spectrogram.shape[0]\n",
    "warped = []\n",
    "for i in range(B):\n",
    "    i_mel = mel_spectrogram[i,:,:].unsqueeze(0)\n",
    "#     print(i_mel.shape)\n",
    "    warped.append(spec_augment_pytorch.spec_augment(i_mel,time_warping_para=80, frequency_masking_para=15,\n",
    "                 time_masking_para=15, frequency_mask_num=2, time_mask_num=3))\n",
    "warped_mel = torch.cat(warped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb3c58b85f8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.display.specshow(warped_mel[0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, 128, 93, 2]' is invalid for input of size 23808",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-280dc9b1d56d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwarped_masked_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec_augment_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/ML_qia2019-1/SpecAugment/spec_augment_pytorch.py\u001b[0m in \u001b[0;36mspec_augment\u001b[0;34m(mel_spectrogram, time_warping_para, frequency_masking_para, time_masking_para, frequency_mask_num, time_mask_num)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# Step 1 : Time warping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mwarped_mel_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_warp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Step 2 : Frequency masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/ML_qia2019-1/SpecAugment/spec_augment_pytorch.py\u001b[0m in \u001b[0;36mtime_warp\u001b[0;34m(spec, W)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0msrc_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint_to_warp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdest_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint_to_warp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdist_to_warp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mwarped_spectro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_flows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_image_warp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_pts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwarped_spectro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py\u001b[0m in \u001b[0;36msparse_image_warp\u001b[0;34m(img_tensor, source_control_point_locations, dest_control_point_locations, interpolation_order, regularization_weight, num_boundaries_points)\u001b[0m\n\u001b[1;32m     46\u001b[0m         regularization_weight)\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mdense_flows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dense_flows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_flows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mwarped_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_image_warp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_flows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/ML_qia2019-1/SpecAugment/sparse_image_warp_pytorch.py\u001b[0m in \u001b[0;36mcreate_dense_flows\u001b[0;34m(flattened_flows, batch_size, image_height, image_width)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# possibly .view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m#     print(flattened_flows.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_flows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[64, 128, 93, 2]' is invalid for input of size 23808"
     ]
    }
   ],
   "source": [
    "warped_masked_spectrogram = spec_augment_pytorch.spec_augment(mel_spectrogram = mel_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(warped_masked_spectrogram.cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(mel_spectrogram.cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_image_warp(img_tensor,\n",
    "                      source_control_point_locations,\n",
    "                      dest_control_point_locations,\n",
    "                      interpolation_order=2,\n",
    "                      regularization_weight=0.0,\n",
    "                      num_boundaries_points=0):\n",
    "    device = img_tensor.device\n",
    "    control_point_flows = (dest_control_point_locations - source_control_point_locations)   \n",
    "    \n",
    "#     clamp_boundaries = num_boundary_points > 0\n",
    "#     boundary_points_per_edge = num_boundary_points - 1\n",
    "    batch_size, image_height, image_width = img_tensor.shape\n",
    "    flattened_grid_locations = get_flat_grid_locations(image_height, image_width, device)\n",
    "\n",
    "    # IGNORED FOR OUR BASIC VERSION...\n",
    "#     flattened_grid_locations = constant_op.constant(\n",
    "#         _expand_to_minibatch(flattened_grid_locations, batch_size), image.dtype)\n",
    "\n",
    "#     if clamp_boundaries:\n",
    "#       (dest_control_point_locations,\n",
    "#        control_point_flows) = _add_zero_flow_controls_at_boundary(\n",
    "#            dest_control_point_locations, control_point_flows, image_height,\n",
    "#            image_width, boundary_points_per_edge)\n",
    "\n",
    "    flattened_flows = interpolate_spline(\n",
    "        dest_control_point_locations,\n",
    "        control_point_flows,\n",
    "        flattened_grid_locations,\n",
    "        interpolation_order,\n",
    "        regularization_weight)\n",
    "\n",
    "    dense_flows = create_dense_flows(flattened_flows, batch_size, image_height, image_width)\n",
    "\n",
    "    warped_image = dense_image_warp(img_tensor, dense_flows)\n",
    "\n",
    "    return warped_image, dense_flows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Export\n",
    "def get_grid_locations(image_height, image_width, device):\n",
    "    y_range = torch.linspace(0, image_height - 1, image_height, device=device)\n",
    "    x_range = torch.linspace(0, image_width - 1, image_width, device=device)\n",
    "    y_grid, x_grid = torch.meshgrid(y_range, x_range)\n",
    "    return torch.stack((y_grid, x_grid), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_grid_locations(grid_locations, image_height, image_width):\n",
    "    return torch.reshape(grid_locations, [image_height * image_width, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_grid_locations(image_height, image_width, device):\n",
    "    y_range = torch.linspace(0, image_height - 1, image_height, device=device)\n",
    "    x_range = torch.linspace(0, image_width - 1, image_width, device=device)\n",
    "    y_grid, x_grid = torch.meshgrid(y_range, x_range)\n",
    "    return torch.stack((y_grid, x_grid), -1).reshape([image_height * image_width, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_flows(flattened_flows, batch_size, image_height, image_width):\n",
    "    # possibly .view\n",
    "    return torch.reshape(flattened_flows, [batch_size, image_height, image_width, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_spline(train_points, train_values, query_points, order, regularization_weight=0.0,):\n",
    "    # First, fit the spline to the observed data.\n",
    "    w, v = solve_interpolation(train_points, train_values, order, regularization_weight)\n",
    "    # Then, evaluate the spline at the query locations.\n",
    "    query_values = apply_interpolation(query_points, train_points, w, v, order)\n",
    "\n",
    "    return query_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_interpolation(train_points, train_values, order, regularization_weight):\n",
    "    device = train_points.device\n",
    "    b, n, d = train_points.shape\n",
    "    k = train_values.shape[-1]\n",
    "\n",
    "    # First, rename variables so that the notation (c, f, w, v, A, B, etc.)\n",
    "    # follows https://en.wikipedia.org/wiki/Polyharmonic_spline.\n",
    "    # To account for python style guidelines we use\n",
    "    # matrix_a for A and matrix_b for B.\n",
    "    \n",
    "    c = train_points\n",
    "    f = train_values.float()\n",
    "    \n",
    "    matrix_a = phi(cross_squared_distance_matrix(c,c), order).unsqueeze(0)  # [b, n, n]\n",
    "#     if regularization_weight > 0:\n",
    "#         batch_identity_matrix = array_ops.expand_dims(\n",
    "#           linalg_ops.eye(n, dtype=c.dtype), 0)\n",
    "#         matrix_a += regularization_weight * batch_identity_matrix\n",
    "\n",
    "    # Append ones to the feature values for the bias term in the linear model.\n",
    "    ones = torch.ones(1, dtype=train_points.dtype, device=device).view([-1, 1, 1])\n",
    "    matrix_b = torch.cat((c, ones), 2).float()  # [b, n, d + 1]\n",
    "\n",
    "    # [b, n + d + 1, n]\n",
    "    left_block = torch.cat((matrix_a, torch.transpose(matrix_b, 2, 1)), 1)\n",
    "\n",
    "    num_b_cols = matrix_b.shape[2]  # d + 1\n",
    "\n",
    "    # In Tensorflow, zeros are used here. Pytorch solve fails with zeros for some reason we don't understand.\n",
    "    # So instead we use very tiny randn values (variance of one, zero mean) on one side of our multiplication.\n",
    "    lhs_zeros = torch.randn((b, num_b_cols, num_b_cols), device=device) / 1e10\n",
    "    right_block = torch.cat((matrix_b, lhs_zeros),\n",
    "                                   1)  # [b, n + d + 1, d + 1]\n",
    "    lhs = torch.cat((left_block, right_block),\n",
    "                           2)  # [b, n + d + 1, n + d + 1]\n",
    "\n",
    "    rhs_zeros = torch.zeros((b, d + 1, k), dtype=train_points.dtype, device=device).float()\n",
    "    rhs = torch.cat((f, rhs_zeros), 1)  # [b, n + d + 1, k]\n",
    "\n",
    "    # Then, solve the linear system and unpack the results.\n",
    "    X, LU = torch.solve(rhs, lhs)\n",
    "    w = X[:, :n, :]\n",
    "    v = X[:, n:, :]\n",
    "\n",
    "    return w, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_squared_distance_matrix(x, y):\n",
    "    \"\"\"Pairwise squared distance between two (batch) matrices' rows (2nd dim).\n",
    "        Computes the pairwise distances between rows of x and rows of y\n",
    "        Args:\n",
    "        x: [batch_size, n, d] float `Tensor`\n",
    "        y: [batch_size, m, d] float `Tensor`\n",
    "        Returns:\n",
    "        squared_dists: [batch_size, n, m] float `Tensor`, where\n",
    "        squared_dists[b,i,j] = ||x[b,i,:] - y[b,j,:]||^2\n",
    "    \"\"\"\n",
    "    x_norm_squared = torch.sum(torch.mul(x, x))\n",
    "    y_norm_squared = torch.sum(torch.mul(y, y))\n",
    "\n",
    "    x_y_transpose = torch.matmul(x.squeeze(0), y.squeeze(0).transpose(0,1))\n",
    "    \n",
    "    # squared_dists[b,i,j] = ||x_bi - y_bj||^2 = x_bi'x_bi- 2x_bi'x_bj + x_bj'x_bj\n",
    "    squared_dists = x_norm_squared - 2 * x_y_transpose + y_norm_squared\n",
    "\n",
    "    return squared_dists.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(r, order):\n",
    "    \"\"\"Coordinate-wise nonlinearity used to define the order of the interpolation.\n",
    "    See https://en.wikipedia.org/wiki/Polyharmonic_spline for the definition.\n",
    "    Args:\n",
    "    r: input op\n",
    "    order: interpolation order\n",
    "    Returns:\n",
    "    phi_k evaluated coordinate-wise on r, for k = r\n",
    "    \"\"\"\n",
    "    EPSILON=torch.tensor(1e-10, device=r.device)\n",
    "    # using EPSILON prevents log(0), sqrt0), etc.\n",
    "    # sqrt(0) is well-defined, but its gradient is not\n",
    "    if order == 1:\n",
    "        r = torch.max(r, EPSILON)\n",
    "        r = torch.sqrt(r)\n",
    "        return r\n",
    "    elif order == 2:\n",
    "        return 0.5 * r * torch.log(torch.max(r, EPSILON))\n",
    "    elif order == 4:\n",
    "        return 0.5 * torch.square(r) * torch.log(torch.max(r, EPSILON))\n",
    "    elif order % 2 == 0:\n",
    "        r = torch.max(r, EPSILON)\n",
    "        return 0.5 * torch.pow(r, 0.5 * order) * torch.log(r)\n",
    "    else:\n",
    "        r = torch.max(r, EPSILON)\n",
    "        return torch.pow(r, 0.5 * order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_interpolation(query_points, train_points, w, v, order):\n",
    "    \"\"\"Apply polyharmonic interpolation model to data.\n",
    "    Given coefficients w and v for the interpolation model, we evaluate\n",
    "    interpolated function values at query_points.\n",
    "    Args:\n",
    "    query_points: `[b, m, d]` x values to evaluate the interpolation at\n",
    "    train_points: `[b, n, d]` x values that act as the interpolation centers\n",
    "                    ( the c variables in the wikipedia article)\n",
    "    w: `[b, n, k]` weights on each interpolation center\n",
    "    v: `[b, d, k]` weights on each input dimension\n",
    "    order: order of the interpolation\n",
    "    Returns:\n",
    "    Polyharmonic interpolation evaluated at points defined in query_points.\n",
    "    \"\"\"\n",
    "    query_points = query_points.unsqueeze(0)\n",
    "    # First, compute the contribution from the rbf term.\n",
    "    pairwise_dists = cross_squared_distance_matrix(query_points.float(), train_points.float())\n",
    "    phi_pairwise_dists = phi(pairwise_dists, order)\n",
    "\n",
    "    rbf_term = torch.matmul(phi_pairwise_dists, w)\n",
    "\n",
    "    # Then, compute the contribution from the linear term.\n",
    "    # Pad query_points with ones, for the bias term in the linear model.\n",
    "    ones = torch.ones_like(query_points[..., :1])\n",
    "    query_points_pad = torch.cat((\n",
    "      query_points,\n",
    "      ones\n",
    "    ), 2).float()\n",
    "    linear_term = torch.matmul(query_points_pad, v)\n",
    "\n",
    "    return rbf_term + linear_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "def dense_image_warp(image, flow):\n",
    "    \"\"\"Image warping using per-pixel flow vectors.\n",
    "    Apply a non-linear warp to the image, where the warp is specified by a dense\n",
    "    flow field of offset vectors that define the correspondences of pixel values\n",
    "    in the output image back to locations in the  source image. Specifically, the\n",
    "    pixel value at output[b, j, i, c] is\n",
    "    images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c].\n",
    "    The locations specified by this formula do not necessarily map to an int\n",
    "    index. Therefore, the pixel value is obtained by bilinear\n",
    "    interpolation of the 4 nearest pixels around\n",
    "    (b, j - flow[b, j, i, 0], i - flow[b, j, i, 1]). For locations outside\n",
    "    of the image, we use the nearest pixel values at the image boundary.\n",
    "    Args:\n",
    "    image: 4-D float `Tensor` with shape `[batch, height, width, channels]`.\n",
    "    flow: A 4-D float `Tensor` with shape `[batch, height, width, 2]`.\n",
    "    name: A name for the operation (optional).\n",
    "    Note that image and flow can be of type tf.half, tf.float32, or tf.float64,\n",
    "    and do not necessarily have to be the same type.\n",
    "    Returns:\n",
    "    A 4-D float `Tensor` with shape`[batch, height, width, channels]`\n",
    "    and same type as input image.\n",
    "    Raises:\n",
    "    ValueError: if height < 2 or width < 2 or the inputs have the wrong number\n",
    "    of dimensions.\n",
    "    \"\"\"\n",
    "    image = image.unsqueeze(3) # add a single channel dimension to image tensor\n",
    "    batch_size, height, width, channels = image.shape\n",
    "    device = image.device\n",
    "\n",
    "    # The flow is defined on the image grid. Turn the flow into a list of query\n",
    "    # points in the grid space.\n",
    "    grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(width, device=device), torch.arange(height, device=device))\n",
    "    \n",
    "    stacked_grid = torch.stack((grid_y, grid_x), dim=2).float()\n",
    "    \n",
    "    batched_grid = stacked_grid.unsqueeze(-1).permute(3, 1, 0, 2)\n",
    "    \n",
    "    query_points_on_grid = batched_grid - flow\n",
    "    query_points_flattened = torch.reshape(query_points_on_grid,\n",
    "                                               [batch_size, height * width, 2])\n",
    "    # Compute values at the query points, then reshape the result back to the\n",
    "    # image grid.\n",
    "    interpolated = interpolate_bilinear(image, query_points_flattened)\n",
    "    interpolated = torch.reshape(interpolated,\n",
    "                                     [batch_size, height, width, channels])\n",
    "    return interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Export\n",
    "def interpolate_bilinear(grid,\n",
    "                         query_points,\n",
    "                         name='interpolate_bilinear',\n",
    "                         indexing='ij'):\n",
    "    \"\"\"Similar to Matlab's interp2 function.\n",
    "    Finds values for query points on a grid using bilinear interpolation.\n",
    "    Args:\n",
    "    grid: a 4-D float `Tensor` of shape `[batch, height, width, channels]`.\n",
    "    query_points: a 3-D float `Tensor` of N points with shape `[batch, N, 2]`.\n",
    "    name: a name for the operation (optional).\n",
    "    indexing: whether the query points are specified as row and column (ij),\n",
    "      or Cartesian coordinates (xy).\n",
    "    Returns:\n",
    "    values: a 3-D `Tensor` with shape `[batch, N, channels]`\n",
    "    Raises:\n",
    "    ValueError: if the indexing mode is invalid, or if the shape of the inputs\n",
    "      invalid.\n",
    "    \"\"\"\n",
    "    if indexing != 'ij' and indexing != 'xy':\n",
    "        raise ValueError('Indexing mode must be \\'ij\\' or \\'xy\\'')\n",
    "\n",
    "\n",
    "    shape = grid.shape\n",
    "    if len(shape) != 4:\n",
    "        msg = 'Grid must be 4 dimensional. Received size: '\n",
    "        raise ValueError(msg + str(grid.shape))\n",
    "\n",
    "    batch_size, height, width, channels = grid.shape\n",
    "\n",
    "    shape = [batch_size, height, width, channels]\n",
    "    query_type = query_points.dtype\n",
    "    grid_type = grid.dtype\n",
    "    grid_device = grid.device\n",
    "\n",
    "    num_queries = query_points.shape[1]\n",
    "\n",
    "    alphas = []\n",
    "    floors = []\n",
    "    ceils = []\n",
    "    index_order = [0, 1] if indexing == 'ij' else [1, 0]\n",
    "    unstacked_query_points = query_points.unbind(2)\n",
    "\n",
    "    for dim in index_order:\n",
    "        queries = unstacked_query_points[dim]\n",
    "\n",
    "        size_in_indexing_dimension = shape[dim + 1]\n",
    "\n",
    "        # max_floor is size_in_indexing_dimension - 2 so that max_floor + 1\n",
    "        # is still a valid index into the grid.\n",
    "        max_floor = torch.tensor(size_in_indexing_dimension - 2, dtype=query_type, device=grid_device)\n",
    "        min_floor = torch.tensor(0.0, dtype=query_type, device=grid_device)\n",
    "        maxx = torch.max(min_floor, torch.floor(queries))\n",
    "        floor = torch.min(maxx, max_floor)\n",
    "        int_floor = floor.long()\n",
    "        floors.append(int_floor)\n",
    "        ceil = int_floor + 1\n",
    "        ceils.append(ceil)\n",
    "\n",
    "        # alpha has the same type as the grid, as we will directly use alpha\n",
    "        # when taking linear combinations of pixel values from the image.\n",
    "        \n",
    "        \n",
    "        alpha = torch.tensor((queries - floor), dtype=grid_type, device=grid_device)\n",
    "        min_alpha = torch.tensor(0.0, dtype=grid_type, device=grid_device)\n",
    "        max_alpha = torch.tensor(1.0, dtype=grid_type, device=grid_device)\n",
    "        alpha = torch.min(torch.max(min_alpha, alpha), max_alpha)\n",
    "\n",
    "        # Expand alpha to [b, n, 1] so we can use broadcasting\n",
    "        # (since the alpha values don't depend on the channel).\n",
    "        alpha = torch.unsqueeze(alpha, 2)\n",
    "        alphas.append(alpha)\n",
    "\n",
    "    flattened_grid = torch.reshape(\n",
    "      grid, [batch_size * height * width, channels])\n",
    "    batch_offsets = torch.reshape(\n",
    "      torch.arange(batch_size, device=grid_device) * height * width, [batch_size, 1])\n",
    "\n",
    "    # This wraps array_ops.gather. We reshape the image data such that the\n",
    "    # batch, y, and x coordinates are pulled into the first dimension.\n",
    "    # Then we gather. Finally, we reshape the output back. It's possible this\n",
    "    # code would be made simpler by using array_ops.gather_nd.\n",
    "    def gather(y_coords, x_coords, name):\n",
    "        linear_coordinates = batch_offsets + y_coords * width + x_coords\n",
    "        gathered_values = torch.gather(flattened_grid.t(), 1, linear_coordinates)\n",
    "        return torch.reshape(gathered_values,\n",
    "                                 [batch_size, num_queries, channels])\n",
    "\n",
    "    # grab the pixel values in the 4 corners around each query point\n",
    "    top_left = gather(floors[0], floors[1], 'top_left')\n",
    "    top_right = gather(floors[0], ceils[1], 'top_right')\n",
    "    bottom_left = gather(ceils[0], floors[1], 'bottom_left')\n",
    "    bottom_right = gather(ceils[0], ceils[1], 'bottom_right')\n",
    "\n",
    "    interp_top = alphas[1] * (top_right - top_left) + top_left\n",
    "    interp_bottom = alphas[1] * (bottom_right - bottom_left) + bottom_left\n",
    "    interp = alphas[0] * (interp_bottom - interp_top) + interp_top\n",
    "\n",
    "    return interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = np.load(\"../audioData/train_process_mel/00716-3-005-m-24-335-sad-sad-sad.npy\")\n",
    "mel_spectrogram = torch.Tensor([mel_spectrogram]*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_warp(spec, W=5):\n",
    "    num_rows = spec.shape[1] ##F\n",
    "    spec_len = spec.shape[2] ##T\n",
    "\n",
    "    y = num_rows // 2\n",
    "    horizontal_line_at_ctr = spec[0][y]\n",
    "    # assert len(horizontal_line_at_ctr) == spec_len\n",
    "\n",
    "    point_to_warp = horizontal_line_at_ctr[random.randrange(W, spec_len-W)]\n",
    "    # assert isinstance(point_to_warp, torch.Tensor)\n",
    "\n",
    "    # Uniform distribution from (0,W) with chance to be up to W negative\n",
    "    dist_to_warp = random.randrange(-W, W)\n",
    "    src_pts = torch.tensor([[[y, point_to_warp]]])\n",
    "    dest_pts = torch.tensor([[[y, point_to_warp + dist_to_warp]]])\n",
    "    warped_spectro, dense_flows = sparse_image_warp(spec, src_pts, dest_pts)\n",
    "    return warped_spectro.squeeze(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_augment(mel_spectrogram, time_warping_para=80, frequency_masking_para=27,\n",
    "                 time_masking_para=100, frequency_mask_num=1, time_mask_num=1):\n",
    "    \"\"\"Spec augmentation Calculation Function.\n",
    "    'SpecAugment' have 3 steps for audio data augmentation.\n",
    "    first step is time warping using Tensorflow's image_sparse_warp function.\n",
    "    Second step is frequency masking, last step is time masking.\n",
    "    # Arguments:\n",
    "      mel_spectrogram(numpy array): audio file path of you want to warping and masking.\n",
    "      time_warping_para(float): Augmentation parameter, \"time warp parameter W\".\n",
    "        If none, default = 80 for LibriSpeech.\n",
    "      frequency_masking_para(float): Augmentation parameter, \"frequency mask parameter F\"\n",
    "        If none, default = 100 for LibriSpeech.\n",
    "      time_masking_para(float): Augmentation parameter, \"time mask parameter T\"\n",
    "        If none, default = 27 for LibriSpeech.\n",
    "      frequency_mask_num(float): number of frequency masking lines, \"m_F\".\n",
    "        If none, default = 1 for LibriSpeech.\n",
    "      time_mask_num(float): number of time masking lines, \"m_T\".\n",
    "        If none, default = 1 for LibriSpeech.\n",
    "    # Returns\n",
    "      mel_spectrogram(numpy array): warped and masked mel spectrogram.\n",
    "    \"\"\"\n",
    "    v = mel_spectrogram.shape[1]\n",
    "    tau = mel_spectrogram.shape[2]\n",
    "\n",
    "    # Step 1 : Time warping\n",
    "    warped_mel_spectrogram = time_warp(mel_spectrogram)\n",
    "\n",
    "    # Step 2 : Frequency masking\n",
    "    for i in range(frequency_mask_num):\n",
    "        f = np.random.uniform(low=0.0, high=frequency_masking_para)\n",
    "        f = int(f)\n",
    "        f0 = random.randint(0, v-f)\n",
    "        warped_mel_spectrogram[:, f0:f0+f, :] = 0\n",
    "\n",
    "    # Step 3 : Time masking\n",
    "    for i in range(time_mask_num):\n",
    "        t = np.random.uniform(low=0.0, high=time_masking_para)\n",
    "        t = int(t)\n",
    "        t0 = random.randint(0, tau-t)\n",
    "        warped_mel_spectrogram[:, :, t0:t0+t] = 0\n",
    "\n",
    "    return warped_mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, 128, 93, 2]' is invalid for input of size 23808",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c19633780c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspec_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-74ff0dff368e>\u001b[0m in \u001b[0;36mspec_augment\u001b[0;34m(mel_spectrogram, time_warping_para, frequency_masking_para, time_masking_para, frequency_mask_num, time_mask_num)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Step 1 : Time warping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mwarped_mel_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_warp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Step 2 : Frequency masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-5c2592e76889>\u001b[0m in \u001b[0;36mtime_warp\u001b[0;34m(spec, W)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msrc_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint_to_warp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdest_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint_to_warp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdist_to_warp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mwarped_spectro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_flows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_image_warp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_pts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwarped_spectro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-47da94e5a41f>\u001b[0m in \u001b[0;36msparse_image_warp\u001b[0;34m(img_tensor, source_control_point_locations, dest_control_point_locations, interpolation_order, regularization_weight, num_boundaries_points)\u001b[0m\n\u001b[1;32m     30\u001b[0m         regularization_weight)\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mdense_flows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dense_flows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_flows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mwarped_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_image_warp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_flows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e26860afb9a8>\u001b[0m in \u001b[0;36mcreate_dense_flows\u001b[0;34m(flattened_flows, batch_size, image_height, image_width)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dense_flows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_flows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# possibly .view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_flows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[64, 128, 93, 2]' is invalid for input of size 23808"
     ]
    }
   ],
   "source": [
    "spec_augment(mel_spectrogram = mel_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
